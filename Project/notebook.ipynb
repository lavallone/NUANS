{"cells":[{"cell_type":"markdown","metadata":{"id":"986S2Td-HJpZ"},"source":["# Extractive Summary as Text Matching on *FairySum*  ðŸ§š"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## Imports & Downloads"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# install the requirements\n","%pip install -r requirements.txt > /dev/null\n","# set to false if you already have the dataset\n","download_dataset = False \n","if download_dataset:\n","    %cd FairySum\n","    !bash download_dataset.sh\n","    %cd .."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from src.hyperparameters import Hparams\n","from sbert.baseline import SentenceBERT\n","from sbert.regression_model import execute_booknlp_pipeline\n","from sbert.regression_model import count_event_sentence\n","from sbert.regression_model import LengthRegressionModel\n","from src.data_module import FairySum_Dataset, FairySum_DataModule\n","from src.model import MatchSum\n","from src.train import train_model\n","\n","import dataclasses\n","from dataclasses import asdict\n","import matplotlib.pyplot as plt\n","import wandb\n","import pprint\n","import json\n","import torchvision\n","import pytorch_lightning as pl\n","import gc\n","from collections import Counter\n","import seaborn as sns\n","from tqdm import tqdm\n","import os\n","import pandas as pd\n","import numpy as np\n","from math import comb\n","import random\n","from datasets import load_metric\n","# reproducibility stuff\n","import numpy as np\n","import random\n","import torch\n","np.random.seed(0)\n","random.seed(0)\n","torch.cuda.manual_seed(0)\n","torch.manual_seed(0)\n","torch.backends.cudnn.deterministic = True  # Note that this Deterministic mode can have a performance impact\n","torch.backends.cudnn.benchmark = False\n","_ = pl.seed_everything(0)\n","# to have a better workflow using notebook https://stackoverflow.com/questions/5364050/reloading-submodules-in-ipython\n","# these commands allow to update the .py codes imported instead of re-importing everything every time.\n","%load_ext autoreload\n","%autoreload 2\n","#%env WANDB_NOTEBOOK_NAME = ./notebook.ipynb\n","gc.collect()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# login wandb to have the online logger. It is really useful since it stores all the plots and evolution of the model\n","# check also https://docs.wandb.ai/guides/integrations/lightning\n","wandb.login()"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## Utilities"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# order a dictionary based on its keys in order to manipulate it better\n","def order_dict(d):\n","    keys = list(d.keys())\n","    keys.sort()\n","    ris = {i: d[i] for i in keys}\n","    return ris\n","\n","# evaluate performance of SBERT\n","def evaluate_performance_baseline(summaries): # the input is only a dictionary with the indices of the summary sentences for each story\n","    rouge_score_list = []\n","    rouge = load_metric(\"rouge\")\n","    for story, indices_list in summaries.items():\n","        summary = ' '.join([FairySum_DataModule.texts[story][i] for i in indices_list])\n","        gold_list = FairySum_DataModule.gold_test[story]\n","        num_gold = len(gold_list)\n","        story_score = {\"rouge_1\" : 0, \"rouge_2\" : 0, \"rouge_L\" : 0}\n","        for g in gold_list:\n","            gold_summary = ' '.join([FairySum_DataModule.texts[story][i] for i in g])\n","            results = rouge.compute(predictions=[summary], references=[gold_summary])\n","            story_score[\"rouge_1\"] += ((results[\"rouge1\"].low.fmeasure+results[\"rouge1\"].mid.fmeasure+results[\"rouge1\"].high.fmeasure)/3)\n","            story_score[\"rouge_2\"] += ((results[\"rouge2\"].low.fmeasure+results[\"rouge2\"].mid.fmeasure+results[\"rouge2\"].high.fmeasure)/3)\n","            story_score[\"rouge_L\"] += ((results[\"rougeL\"].low.fmeasure+results[\"rougeL\"].mid.fmeasure+results[\"rougeL\"].high.fmeasure)/3)\n","        for k in story_score.keys():\n","            story_score.update({k : story_score[k] / num_gold})\n","        r1 = story_score[\"rouge_1\"] / num_gold\n","        r2 = story_score[\"rouge_2\"] / num_gold\n","        r3 = story_score[\"rouge_L\"] / num_gold\n","                \n","        print(f\"Story: {story}\")\n","        print(f\"ROUGE-1: {r1:.3f}\")\n","        print(f\"ROUGE-2: {r2:.3f}\")\n","        print(f\"ROUGE-L: {r3:.3f}\")\n","        print(\"-------------------------------------------------\")\n","        rouge_score_list.append(story_score)\n","    print()\n","    all_rouge_1 = [e[\"rouge_1\"] for e in rouge_score_list]\n","    print(f\"AVERAGE ROUGE-1 for all the stories: {np.array(all_rouge_1).mean()}\")\n","    print()\n","    all_rouge_2 = [e[\"rouge_2\"] for e in rouge_score_list]\n","    print(f\"AVERAGE ROUGE-2 for all the stories: {np.array(all_rouge_2).mean()}\")\n","    print()\n","    all_rouge_L = [e[\"rouge_L\"] for e in rouge_score_list]\n","    print(f\"AVERAGE ROUGE-L for all the stories: {np.array(all_rouge_L).mean()}\")\n","    print()\n","\n","# evaluate performance of my models\n","def evaluate_performance(model, data):\n","    model.eval()\n","    device = model.device\n","    dataset = data.val_dataloader()\n","\n","    with torch.no_grad():\n","        rouge_score_list = []\n","        for batch in tqdm(iter(dataset)): # tqdm ci permette di visualizzare il progresso della lettura del dataset\n","            batch[\"text\"] = {k : v.to(device) for k,v in batch[\"text\"].items()}\n","            batch[\"candidates\"] = [{k : v.to(device) for k,v in e.items()} for e in batch[\"candidates\"]]\n","            best_candidates_list = model.predict(batch)\n","            \n","            # COMPUTE ROUGE SCORES\n","            rouge = load_metric(\"rouge\")\n","            for story, best_candidate in list(zip(batch[\"id\"], best_candidates_list)):\n","                gold_list = FairySum_DataModule.gold_test[story]\n","                num_gold = len(gold_list)\n","                story_score = {\"rouge_1\" : 0, \"rouge_2\" : 0, \"rouge_L\" : 0}\n","                for g in gold_list:\n","                    gold_summary = ' '.join([FairySum_DataModule.texts[story][i] for i in g])\n","                    results = rouge.compute(predictions=[best_candidate], references=[gold_summary])\n","                    story_score[\"rouge_1\"] += ((results[\"rouge1\"].low.fmeasure+results[\"rouge1\"].mid.fmeasure+results[\"rouge1\"].high.fmeasure)/3)\n","                    story_score[\"rouge_2\"] += ((results[\"rouge2\"].low.fmeasure+results[\"rouge2\"].mid.fmeasure+results[\"rouge2\"].high.fmeasure)/3)\n","                    story_score[\"rouge_L\"] += ((results[\"rougeL\"].low.fmeasure+results[\"rougeL\"].mid.fmeasure+results[\"rougeL\"].high.fmeasure)/3)\n","                for k in story_score.keys():\n","                    story_score.update({k : story_score[k] / num_gold})\n","                r1 = story_score[\"rouge_1\"] / num_gold\n","                r2 = story_score[\"rouge_2\"] / num_gold\n","                r3 = story_score[\"rouge_L\"] / num_gold\n","                \n","                print(f\"Story: {story}\")\n","                print(f\"ROUGE-1: {r1:.3f}\")\n","                print(f\"ROUGE-2: {r2:.3f}\")\n","                print(f\"ROUGE-L: {r3:.3f}\")\n","                print(\"-------------------------------------------------\")\n","                rouge_score_list.append(story_score)\n","        print()\n","        all_rouge_1 = [e[\"rouge_1\"] for e in rouge_score_list]\n","        print(f\"AVERAGE ROUGE-1 for all the stories: {np.array(all_rouge_1).mean()}\")\n","        print()\n","        all_rouge_2 = [e[\"rouge_2\"] for e in rouge_score_list]\n","        print(f\"AVERAGE ROUGE-2 for all the stories: {np.array(all_rouge_2).mean()}\")\n","        print()\n","        all_rouge_L = [e[\"rouge_L\"] for e in rouge_score_list]\n","        print(f\"AVERAGE ROUGE-L for all the stories: {np.array(all_rouge_L).mean()}\")\n","        print()\n","                \n","            "]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## Preprocessing"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### Candidates Extraction"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["#### **Phase 1** - *output summaries length regression*"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# training the regression model for predicting the best summary length (i.e. the number of sentences to be extracted)\n","\n","\"\"\"\n","    - The original texts length are saved in 'original_length.json' file. \n","    - The golden output lengths (that are the computed average lenghts of the manually annotated summaries by the students) are saved \n","      in the 'gold_length.json' file.\n","      We now need to compute a quantity which quantifies somehow the concentration of events in each story. We leverage the BookNLP library \n","      to do so. The adopted strategy is very simple: once the library has detected the events, we count the number of unique sentences that\n","      contain at least one EVENT (it means that these sentences are relevant for the story).\n","\"\"\"\n","\n","texts = json.load(open(\"data/texts.json\",\"r\"))\n","\n","booknlp_already_computed = True if os.path.isdir(\"data/booknlp_processed_texts\")==True else False\n","if not booknlp_already_computed:\n","  from booknlp.booknlp import BookNLP\n","  !python -m spacy download en_core_web_sm > /dev/null # needed for the BookNLP library\n","  model_params={\"pipeline\":\"entity,event\", \"model\":\"big\"}\n","  booknlp = BookNLP(\"en\", model_params)\n","  texts = json.load(open(\"data/texts.json\",\"r\"))\n","  execute_booknlp_pipeline(booknlp, texts)\n","  !rm \"data/current_story.txt\"\n","  !rm -r \"data/current\"\n","  \n","events_already_computed = True\n","if not events_already_computed:\n","  count_event_sentence(\"data/booknlp_processed_texts/\", texts)\n","\n","# starting the regression phase\n","# since we miss one golden summary --> 'bn_02975525n_Rothschild_s Violin', we need to remove it from 'events' and from 'original_lenghts'\n","events = json.load(open(\"data/events.json\",\"r\"))\n","del events[\"bn_02975525n\"]\n","events = order_dict(events)\n","\n","gold_lengths = json.load(open(\"data/gold_length.json\",\"r\"))\n","gold_lengths = order_dict(gold_lengths)\n","original_lenghts = json.load(open(\"data/original_length.json\",\"r\"))\n","del original_lenghts[\"bn_02975525n\"]\n","original_lenghts = order_dict(original_lenghts)\n","\n","# instantiate the regression model for predicting  the output length of our generated extractive summaries\n","LengthRegressionModel = LengthRegressionModel(gold_lengths, original_lenghts, events)\n","# LengthRegressionModel.plot() # if we want to plot the regression curve\n","LengthRegressionModel.fit()\n","predictions = LengthRegressionModel.predict()\n"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["#### **Phase 2** - *sentences extraction*"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["hparams = asdict(Hparams())\n","hparams[\"sbert_mode\"] = \"extraction\"\n","sbert = SentenceBERT(hparams, predictions)\n","\n","texts = json.load(open(\"data/texts.json\",\"r\"))\n","extracted_sentences = sbert(texts) # we receive a dictionary with the extracted sentences indices for each story"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["#### **Phase 3** - *sentences selection*"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# selection strategy\n","\n","k_range = hparams[\"k_range\"]\n","pick_random_n = hparams[\"pick_random_n\"]\n","\n","candidates_dict = {}\n","for k,v in extracted_sentences.items():\n","    candidates_list = []\n","    candidates_list.append(v)\n","    n = len(v)\n","    for i in range(n-k_range, n):\n","        #c = comb(n, i) # total number of combinations\n","        random_idx = []\n","        for _ in range(pick_random_n): # how many random combinations?\n","            candidates_list.append([v[i] for i in sorted(random.sample(range(n), i))])\n","            \n","    candidates_dict[k] = candidates_list\n","    \n","# save them\n","json.dump(candidates_dict, open(\"data/candidates/candidates.json\", \"w\"))"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["#### Compute candidates ROUGE scores\n","*We need it for the training phase.*"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# we need to know wich are the training texts\n","train_keys = []\n","for f in os.listdir(\"FairySum/texts/train/\"):\n","    k = \"_\".join(f.split(\"_\")[:2])\n","    train_keys.append(k)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# starting from the candidates.json file\n","candidates_dict = json.load(open(\"data/candidates/candidates.json\", \"r\"))\n","\n","gold_dict = json.load(open(\"data/gold/gold.json\", \"r\"))\n","texts = json.load(open(\"data/texts.json\", \"r\"))\n","\n","# we need to build these two dictionaries\n","ROUGE_predictions = {}\n","ROUGE_references = {}\n","\n","for k,v in candidates_dict.items():\n","    if k not in train_keys:\n","        continue\n","    original_story = texts[k]\n","    for candidate_indices in v:\n","        text_candidate = [original_story[i] for i in candidate_indices]\n","        if k in ROUGE_predictions.keys():\n","            ROUGE_predictions[k].append(\" \".join(text_candidate))\n","        else:\n","            ROUGE_predictions[k] = [\" \".join(text_candidate)]\n","\n","for k,v in gold_dict.items():\n","    original_story = texts[k]\n","    for gold_indices in v:\n","        text_gold = [original_story[i] for i in gold_indices]\n","        if k in ROUGE_references.keys():\n","            ROUGE_references[k].append(\" \".join(text_gold))\n","        else:\n","            ROUGE_references[k] = [\" \".join(text_gold)]\n","\n","# now we can start computing the ROUGE scores\n","rouge = load_metric(\"rouge\")\n","scores = {}\n","for story, candidates in ROUGE_predictions.items():\n","    for candidate in tqdm(candidates):\n","        score = 0\n","        for gold in ROUGE_references[story]:\n","            results = rouge.compute(predictions=[candidate], references=[gold])\n","            score += ((results[\"rougeL\"].low.fmeasure+results[\"rougeL\"].mid.fmeasure+results[\"rougeL\"].high.fmeasure)/3)\n","        score = score/len(ROUGE_references[story])\n","        if story in scores.keys():\n","            scores[story].append(score)\n","        else:\n","            scores[story] = [score]\n","            \n","# save the scores dictionary into the 'candodates' folder\n","json.dump(scores, open(\"data/candidates/scores.json\", \"w\"))"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### Abstractive Summaries"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["We use  the SOTA **PEGASUS model** for computing the abstractive summaries needed  for  training. We simply download a pretrained model and use it as it is (*plug-and-play*)."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["from transformers import PegasusForConditionalGeneration, PegasusTokenizer\n","\n","# create the tokenizer\n","tokenizer = PegasusTokenizer.from_pretrained(\"google/pegasus-large\")\n","\n","# load the model\n","torch_device = 'cuda' if torch.cuda.is_available() else 'cpu'\n","model = PegasusForConditionalGeneration.from_pretrained(\"google/pegasus-large\").to(torch_device)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# we need to know wich are the training texts\n","train_keys = []\n","for f in os.listdir(\"FairySum/texts/train/\"):\n","    k = \"_\".join(f.split(\"_\")[:2])\n","    train_keys.append(k)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["texts = json.load(open(\"data/texts.json\",\"r\"))\n","abstractives = {}\n","\n","# we make inference on this model\n","model.eval()\n","with torch.no_grad():\n","    for i, (k,v) in tqdm(enumerate(texts.items())):\n","        if k not in train_keys:\n","            continue\n","        # we need to have the text available as a whole\n","        text = \" \".join([e+\"\\n\" for e in v])\n","        # create tokens batch\n","        batch = tokenizer.prepare_seq2seq_batch(text, truncation=True, padding=\"longest\", return_tensors=\"pt\").to(torch_device)\n","        # summary in tokens\n","        summary = model.generate(**batch)\n","        # (we need to decode it)\n","        abstractive_output = tokenizer.batch_decode(summary, skip_special_tokens=True)[0]\n","        abstractives[k] = abstractive_output\n","        ##########################\n","        ####   free GPU RAM   ####\n","        ##########################\n","        del batch\n","        del summary\n","        del abstractive_output\n","        torch.cuda.empty_cache()\n","        ##########################\n","        \n","# save the dictionary\n","json.dump(abstractives, open(\"data/abstractives/abstractives.json\", \"w\"))"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["> We use *pegasus-large* pretrainied model because it outputs longer summaries, but since it is trained mainly on news articles which are shorter than our dataset, the output summaries are certainly not ideal. This can be a research direction to follow for future improvements of the overall method. For this mini-project we keep the model as it is, hoping in the achievement of decent results."]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### Gold Summaries processing"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# we need to know wich are the training texts\n","train_keys = []\n","for f in os.listdir(\"FairySum/texts/train/\"):\n","    k = \"_\".join(f.split(\"_\")[:2])\n","    train_keys.append(k)"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["# we want to create a dictionary for each gold summary with only the sentences indeces\n","# of TRAIN texts for the training phase\n","gold = {}\n","for file_name in [\"FairySum/gold/\"+x for x in os.listdir(\"FairySum/gold/\")]:\n","  key = file_name.split(\"/\")[-1]\n","  key = \"_\".join(key.split(\"_\")[:2])\n","  if key not in train_keys:\n","    continue\n","  f = open(file_name, 'r')\n","  lines = f.readlines()\n","  text = []\n","  for l in lines:\n","    i = l.index(\":\")\n","    text.append(int(l[:i]))\n","  if key in gold.keys(): # if we have more than one gold summary we apppend to the list\n","    gold[key].append(text)\n","  else:\n","    gold[key] = [text]\n","    \n","# save the dictionary\n","json.dump(gold, open(\"data/gold/gold.json\", \"w\"))\n","\n","# and of the TEST texts for the evaluation performances pipeline\n","gold_test = {}\n","for file_name in [\"FairySum/gold/\"+x for x in os.listdir(\"FairySum/gold/\")]:\n","  key = file_name.split(\"/\")[-1]\n","  key = \"_\".join(key.split(\"_\")[:2])\n","  if key in train_keys:\n","    continue\n","  f = open(file_name, 'r')\n","  lines = f.readlines()\n","  text = []\n","  for l in lines:\n","    i = l.index(\":\")\n","    text.append(int(l[:i]))\n","  if key in gold_test.keys(): # if we have more than one gold summary we apppend to the list\n","    gold_test[key].append(text)\n","  else:\n","    gold_test[key] = [text]\n","    \n","# save the dictionary\n","json.dump(gold_test, open(\"data/gold/gold_test.json\", \"w\"))"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## Dataset"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["hparams = asdict(Hparams())"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["FairySum_Data = FairySum_DataModule(hparams)\n","# to setup it takes 0.2s (but because the most demanding operations are made at \"batch time\")\n","FairySum_Data.setup()\n","print(len(FairySum_Data.data_train)) # -->  75 stories\n","print(len(FairySum_Data.data_test)) # -->  16 stories\n","print(\"TOTAL: \"+str(len(FairySum_Data.data_train)+len(FairySum_Data.data_test))+\" Fairy Tales and Short Stories\")"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["batch1 = next(iter(FairySum_Data.train_dataloader()))\n","batch2 = next(iter(FairySum_Data.val_dataloader()))"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"-y0tDVGRwJLp"},"source":["## Model"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### Finetuning on FairySum"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["user_name = \"lavallone\"\n","project_name = \"NUANS_project\"\n","version_name = \"prova\"\n","run = wandb.init(entity=user_name, project=project_name, name = version_name, mode = \"online\")\n","\n","hparams = asdict(Hparams())\n","data = FairySum_DataModule(hparams)\n","model = MatchSum(hparams)\n","trainer = train_model(data, model, experiment_name = version_name, \\\n","    patience=5, metric_to_monitor=\"val_ROUGE\", mode=\"max\", epochs = 1)\n","\n","wandb.finish()"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## Evaluation"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### Baseline\n","> Computing the performances of the *baseline* model. These will be the values to beat."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["hparams = asdict(Hparams())\n","hparams[\"sbert_mode\"] = \"evaluation\"\n","sbert = SentenceBERT(hparams)\n","\n","texts = json.load(open(\"data/texts.json\",\"r\"))\n","# we only give the texts from the TEST set\n","test_keys = []\n","for f in os.listdir(\"FairySum/texts/test/\"):\n","    k = \"_\".join(f.split(\"_\")[:2])\n","    test_keys.append(k)\n","test_texts = {k:v for k,v in texts.items() if k in test_keys}\n","\n","summaries = sbert(test_texts)\n","evaluate_performance_baseline(summaries)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### MatchSum\n","> Computing the performances of my solutions."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["load_ckpt = True\n","if load_ckpt:\n","    best_ckpt = \"models/prova-epoch=00-val_ROUGE=0.6224.ckpt\"\n","    model = MatchSum.load_from_checkpoint(best_ckpt, strict=False, device = \"cuda\" if torch.cuda.is_available() else \"cpu\")\n","# if we want to test without training we need to setup the data\n","trained = False \n","if not trained:\n","    hparams = asdict(Hparams())\n","    data = FairySum_DataModule(hparams)\n","    data.setup()\n","\n","evaluate_performance(model, data)"]},{"cell_type":"code","execution_count":11,"metadata":{},"outputs":[{"ename":"ModuleNotFoundError","evalue":"No module named 'model'","output_type":"error","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)","Cell \u001b[0;32mIn [11], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[39mimport\u001b[39;00m \u001b[39mtorch\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m ckpt \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mload(\u001b[39m'\u001b[39m\u001b[39mMatchSum_cnndm_bert.ckpt\u001b[39m\u001b[39m'\u001b[39m)\n","File \u001b[0;32m~/miniconda3/envs/sappia/lib/python3.10/site-packages/torch/serialization.py:713\u001b[0m, in \u001b[0;36mload\u001b[0;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[1;32m    711\u001b[0m             \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39mjit\u001b[39m.\u001b[39mload(opened_file)\n\u001b[1;32m    712\u001b[0m         \u001b[39mreturn\u001b[39;00m _load(opened_zipfile, map_location, pickle_module, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mpickle_load_args)\n\u001b[0;32m--> 713\u001b[0m \u001b[39mreturn\u001b[39;00m _legacy_load(opened_file, map_location, pickle_module, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mpickle_load_args)\n","File \u001b[0;32m~/miniconda3/envs/sappia/lib/python3.10/site-packages/torch/serialization.py:930\u001b[0m, in \u001b[0;36m_legacy_load\u001b[0;34m(f, map_location, pickle_module, **pickle_load_args)\u001b[0m\n\u001b[1;32m    928\u001b[0m unpickler \u001b[39m=\u001b[39m UnpicklerWrapper(f, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mpickle_load_args)\n\u001b[1;32m    929\u001b[0m unpickler\u001b[39m.\u001b[39mpersistent_load \u001b[39m=\u001b[39m persistent_load\n\u001b[0;32m--> 930\u001b[0m result \u001b[39m=\u001b[39m unpickler\u001b[39m.\u001b[39;49mload()\n\u001b[1;32m    932\u001b[0m deserialized_storage_keys \u001b[39m=\u001b[39m pickle_module\u001b[39m.\u001b[39mload(f, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mpickle_load_args)\n\u001b[1;32m    934\u001b[0m offset \u001b[39m=\u001b[39m f\u001b[39m.\u001b[39mtell() \u001b[39mif\u001b[39;00m f_should_read_directly \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n","File \u001b[0;32m~/miniconda3/envs/sappia/lib/python3.10/site-packages/torch/serialization.py:746\u001b[0m, in \u001b[0;36m_legacy_load.<locals>.UnpicklerWrapper.find_class\u001b[0;34m(self, mod_name, name)\u001b[0m\n\u001b[1;32m    744\u001b[0m     \u001b[39mexcept\u001b[39;00m \u001b[39mKeyError\u001b[39;00m:\n\u001b[1;32m    745\u001b[0m         \u001b[39mpass\u001b[39;00m\n\u001b[0;32m--> 746\u001b[0m \u001b[39mreturn\u001b[39;00m \u001b[39msuper\u001b[39;49m()\u001b[39m.\u001b[39;49mfind_class(mod_name, name)\n","\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'model'"]}],"source":["import torch\n","ckpt = torch.load('MatchSum_cnndm_bert.ckpt')"]}],"metadata":{"accelerator":"GPU","colab":{"collapsed_sections":["sYnPaW3xqB76","o_pOiPmDqB78","UM9r2JPLqB79","dWYN7dXdqB7_","-6AfNu4BqB8A","8qebcYffqB8A","jGyjEQYeqB8B","3rN7DegMJpJB","P8ThCPp6Jt5c","7_K0zpGyqB8H","iCFsRmN9qB8I"],"provenance":[],"toc_visible":true},"gpuClass":"standard","kernelspec":{"display_name":"sappia","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.4"},"vscode":{"interpreter":{"hash":"5f06e66338bb5301debc8a4cff3b178f3ee2a0c1aca00670585ce1ed8b6c95da"}}},"nbformat":4,"nbformat_minor":0}
