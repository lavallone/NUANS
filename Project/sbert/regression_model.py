import os
import shutil
import pandas as pd
import json
from sklearn.svm import SVR
from sklearn.pipeline import make_pipeline
from sklearn.preprocessing import StandardScaler
import numpy as np
import seaborn as sns

# compute the booknlp pipeline for extracting EVENTS by producing the '.tokens' files for each story
def execute_booknlp_pipeline(booknlp, texts):
    os.mkdir("data/booknlp_processed_texts")
    for k,v in texts.items():
        v = [e+"\n" for e in v]
        open("data/current_story.txt", "w").writelines(v) # create the current (temporary) .txt file
        
        # BookNLP pipeline
        booknlp.process("data/current_story.txt", "data/current", "current")
        shutil.copy("data/current/current.tokens", "data/booknlp_processed_texts")
        os.rename("data/booknlp_processed_texts/current.tokens", "data/booknlp_processed_texts/"+k+".tokens")

# compute "events concentration" for each story (it's the second quantity used for predicting the output summary length)
def count_event_sentence(tokens_path_dir, texts):
    events = {}
    for k,_ in texts.items():
        # we read the .tokens file generated by booknlp
        df_tokens_file = pd.read_csv(tokens_path_dir+k+".tokens", delimiter="\t", engine="python", encoding="utf-8", on_bad_lines="skip")#"warn")
        real_events = df_tokens_file.loc[df_tokens_file["event"] == "EVENT"] # we pick the EVENTS
        sentences = real_events.sentence_ID.tolist() # we create a list of the sentence IDs where the EVENTS appear!
        # we update the events dictionary with the number of sentences that have at least one EVENT
        sentences_number = len(list(set(sentences)))
        events[k] = sentences_number
    json.dump(events, open("data/events.json", "w"))

class LengthRegressionModel:
    """
        we develop a regression model which as to infer the 'best' summary output length based on two inputs:
        1) original story length.
        2) numbers of events that happen in the story.
    """
    def __init__(self, gold_lengths, original_lenghts, events):
        super(LengthRegressionModel, self).__init__()
        self.gold_lengths = gold_lengths
        self.original_lenghts = original_lenghts
        self.events = events
        self.Y, self.X1, self.X2 = [],[],[]
        for y, (x1,x2) in list( zip( self.gold_lengths.values(), list(zip(self.original_lenghts.values(), self.events.values()))) ):
            self.Y.append(y)
            self.X1.append(x1)
            self.X2.append(x2)
    
    def fit(self):
        # create X for regression model --> (90, 2)
        X = []
        for x1,x2 in list(zip(self.X1,self.X2)):
            X.append([x1,x2])
        X = np.array(X)
        # define also y --> (90,)
        y = np.array(self.Y)
        
        regression_model= make_pipeline(StandardScaler(), SVR(kernel="poly", degree=6))
        regression_model.fit(X, y)
        self.length_regression_model = regression_model
    
    def predict(self):
        ris = {}
        # we reinstantiate them because in the training phase we didn't consider the 'bn_02975525n_Rothschild_s Violin' story
        events = json.load(open("data/events.json","r"))
        original_lenghts = json.load(open("data/original_length.json","r"))
        for k in events.keys():
            x1 = original_lenghts[k]
            x2 = events[k]
            ris[k] = round((self.length_regression_model.predict([[x1,x2]]))[0])
        return ris
        
    
    def plot(self):
        #data={"Name" : original_lenghts.keys(), 'OriginalLength' : X1, 'Events' : X2, 'OutputLength' : Y}
        inputs=["Original Length" for i in range(1,91)]+["Concentration of Events" for i in range(1,91)]
        data={"Name" : list(self.original_lenghts.keys())+list(self.original_lenghts.keys()), '{X1, X2}' : self.X1+self.X2, 'Input variables {X1, X2}:' : inputs, 'Output Length' : self.Y+self.Y}
        df=pd.DataFrame(data)
        sns.lmplot(df, x="{X1, X2}", y="Output Length", order=6, ci=30, scatter_kws={"s": 100}, hue="Input variables {X1, X2}:")